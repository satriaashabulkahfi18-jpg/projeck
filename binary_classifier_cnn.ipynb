import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import os
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# Configuration
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
LEARNING_RATE = 0.001
EPOCHS = 50

# Dataset paths - adjust these paths according to your dataset structure
cassava_path = "dataset/cassava_leaves"  # Directory containing cassava leaf images
non_cassava_path = "dataset/non_cassava"  # Directory containing non-cassava images

def create_binary_data_generators(cassava_path, non_cassava_path):
    """Create data generators for binary classification (cassava vs non-cassava)"""

    # Create directories if they don't exist
    os.makedirs(cassava_path, exist_ok=True)
    os.makedirs(non_cassava_path, exist_ok=True)

    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.15,
        zoom_range=0.15,
        horizontal_flip=True,
        fill_mode='nearest',
        validation_split=0.2
    )

    # Create a combined data generator
    # We'll need to organize data into subdirectories for flow_from_directory
    combined_data_path = "dataset/binary_classification"

    # Create train/cassava, train/non_cassava, val/cassava, val/non_cassava structure
    for split in ['train', 'val']:
        for category in ['cassava', 'non_cassava']:
            os.makedirs(os.path.join(combined_data_path, split, category), exist_ok=True)

    print("ðŸ“‚ Creating binary classification data generators...")
    print("Note: Please organize your data as follows:")
    print(f"- {cassava_path}/: cassava leaf images")
    print(f"- {non_cassava_path}/: non-cassava images")

    # For now, create generators assuming data is already organized
    train_generator = train_datagen.flow_from_directory(
        os.path.join(combined_data_path, 'train'),
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='binary',
        subset='training',
        shuffle=True
    )

    val_generator = train_datagen.flow_from_directory(
        os.path.join(combined_data_path, 'val'),
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='binary',
        subset='validation',
        shuffle=False
    )

    return train_generator, val_generator

def create_binary_vgg_model():
    """Create VGG16-based binary classification model"""

    print("ðŸ—ï¸ Creating binary VGG16 model...")

    # Base model VGG16
    base_model = VGG16(
        weights='imagenet',
        include_top=False,
        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)
    )

    # Freeze base model
    base_model.trainable = False

    # Custom head for binary classification
    inputs = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))
    x = base_model(inputs, training=False)
    x = GlobalAveragePooling2D()(x)
    x = Dropout(0.3)(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)
    outputs = Dense(1, activation='sigmoid')(x)  # Binary classification

    model = Model(inputs, outputs)

    return model

def train_binary_model():
    """Train the binary classification model"""

    # Create data generators
    train_gen, val_gen = create_binary_data_generators(cassava_path, non_cassava_path)

    # Create model
    model = create_binary_vgg_model()

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC()]
    )

    print("\nðŸ“‹ Binary Classification Model Architecture:")
    model.summary()

    # Setup callbacks
    checkpoint_filepath = 'model/binary_classifier.h5'

    callbacks = [
        ModelCheckpoint(
            checkpoint_filepath,
            monitor='val_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
        ),
        EarlyStopping(
            monitor='val_loss',
            patience=8,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.3,
            patience=3,
            min_lr=0.00001,
            verbose=1
        )
    ]

    print(f"\nðŸš€ Starting binary classification training...")
    print(f"ðŸŽ¯ Target epochs: {EPOCHS}")
    print(f"ðŸ“Š Batch size: {BATCH_SIZE}")
    print(f"ðŸ§  Learning rate: {LEARNING_RATE}")

    history = model.fit(
        train_gen,
        epochs=EPOCHS,
        validation_data=val_gen,
        callbacks=callbacks,
        verbose=1
    )

    return model, history

def plot_training_history(history):
    """Plot training history for binary classification"""

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

    # Plot accuracy
    ax1.plot(history.history['accuracy'], label='Training Accuracy')
    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')
    ax1.set_title('Model Accuracy')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend()
    ax1.grid(True)

    # Plot loss
    ax2.plot(history.history['loss'], label='Training Loss')
    ax2.plot(history.history['val_loss'], label='Validation Loss')
    ax2.set_title('Model Loss')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend()
    ax2.grid(True)

    # Plot AUC
    ax3.plot(history.history['auc'], label='Training AUC')
    ax3.plot(history.history['val_auc'], label='Validation AUC')
    ax3.set_title('Model AUC')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('AUC')
    ax3.legend()
    ax3.grid(True)

    # Plot learning rate
    if 'lr' in history.history:
        ax4.plot(history.history['lr'], label='Learning Rate')
        ax4.set_title('Learning Rate Schedule')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Learning Rate')
        ax4.set_yscale('log')
        ax4.legend()
        ax4.grid(True)

    plt.tight_layout()
    plt.show()

def evaluate_model(model, val_gen):
    """Evaluate the trained model"""

    print("\nðŸ” Evaluating model...")

    # Get predictions
    val_gen.reset()
    predictions = model.predict(val_gen)
    y_pred = (predictions > 0.5).astype(int).flatten()
    y_true = val_gen.classes

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Cassava', 'Non-Cassava'],
                yticklabels=['Cassava', 'Non-Cassava'])
    plt.title('Confusion Matrix - Binary Classification')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

    # Classification report
    print("\nðŸ“Š Classification Report:")
    print(classification_report(y_true, y_pred, target_names=['Cassava', 'Non-Cassava']))

    # Calculate metrics
    tn, fp, fn, tp = cm.ravel()
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    print(".4f")
    print(".4f")
    print(".4f")
    print(".4f")

# Main execution
if __name__ == "__main__":
    print("ðŸŒ¿ Binary Cassava Leaf Classification System")
    print("=" * 50)

    # Train model
    model, history = train_binary_model()

    # Plot training history
    plot_training_history(history)

    # Evaluate model
    _, val_gen = create_binary_data_generators(cassava_path, non_cassava_path)
    evaluate_model(model, val_gen)

    print("\nâœ… Binary classification training completed!")
    print("ðŸ’¾ Model saved as: model/binary_classifier.h5")